---
title: "Stat 521 Predictive Modeling Final Project Writeup"
author: '[Yilin Gao, Wenqi Cheng, Liyu Gong, Mengrui Yin]'
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

```{r packages, echo=FALSE}
suppressMessages(library(dplyr))
suppressMessages(library(corrplot))
suppressMessages(library(forcats))
suppressMessages(library(randomForest))
suppressMessages(library(gbm))
suppressMessages(library(knitr))
suppressMessages(library(glmnet))
suppressMessages(library(ggplot2))
```

```{r load data, echo = FALSE}
load("ames_train.Rdata")
load("ames_test.Rdata")
load("ames_validation.Rdata")
```

```{r data cleaing, echo=FALSE}
train = ames_train %>% 
  mutate(datset = "train")
test = ames_test %>%
  mutate(datset = "test")
valid = ames_validation %>%
  mutate(datset = "valid")

dat = train %>% 
  rbind(., test) %>%
  rbind(., valid)

findNA = function(col){
  numNA = sum(is.na(col) | col == "")
  type = class(col)
  level = paste(as.character(levels(col)), collapse = ";")
  large_0 = sum(table(col) > 0)
  return(c(numNA, type, level, large_0))
}

train_info = lapply(dat, findNA)
train_info = as.data.frame(do.call(rbind, train_info), stringsAsFactors = F)
colnames(train_info) = c("numNA", "class", "level")
train_info$numNA = as.numeric(train_info$numNA)
train_info$missing_ratio = train_info$numNA/nrow(ames_train)
train_info$names = rownames(train_info)

features_drop = rownames(train_info)[train_info$missing_ratio > 0.5]
features_remain = rownames(train_info)[train_info$missing_ratio <= 0.5]

dat1 = dat[, c(features_remain)]

# Bsmt.Exposure
dat1$Bsmt.Exposure[dat1$Bsmt.Exposure == "" &
                     !is.na(dat1$Bsmt.Exposure)] = names(sort(table(dat1$Bsmt.Exposure), 
                                                              decreasing = T))[1]
dat1$Mas.Vnr.Type[dat1$Mas.Vnr.Type == ""] = "None"
dat1$Electrical[dat1$Electrical == ""] = names(sort(table(dat1$Electrical),
                                                    decreasing = T))[1]
dat1$BsmtFin.Type.2[dat1$BsmtFin.Type.2 == "" &
                      !is.na(dat1$BsmtFin.Type.2)] = NA

dat2 = dat1 %>%
  dplyr::select(-Garage.Yr.Blt) %>%
  filter(PID != 903426160) %>%
  mutate(MS.SubClass = as.factor(MS.SubClass),
         Lot.Frontage = ifelse(is.na(Lot.Frontage), 0, Lot.Frontage),
         #Fireplace.Qu = fct_explicit_na(Fireplace.Qu, "Unknown"),
         #Fireplace.Qu = as.numeric(factor(Fireplace.Qu, 
         #                                  levels = c("Unknown", "Po", "Fa", 
         #                                             "TA", "Gd", "Ex"))),
         Garage.Cond = fct_explicit_na(Garage.Cond, "Unknown"),
         Garage.Cond = as.numeric(factor(Garage.Cond, 
                                         levels = c("Unknown", "Po", "Fa", 
                                                    "TA", "Gd", "Ex"))),
         Garage.Qual = fct_explicit_na(Garage.Qual, "Unknown"),
         Garage.Qual = as.numeric(factor(Garage.Qual, 
                                         levels = c("Unknown", "Po", "Fa", 
                                                    "TA", "Gd", "Ex"))),
         Garage.Type = fct_explicit_na(Garage.Type, "Unknown"),
         Garage.Finish = fct_explicit_na(Garage.Finish, "Unknown"),
         Garage.Finish = as.numeric(factor(Garage.Finish, 
                                           levels = c("Unknown", "Unf", 
                                                      "RFn", "Fin"))),
         Bsmt.Qual = fct_explicit_na(Bsmt.Qual, "Unknown"),
         Bsmt.Qual = as.numeric(factor(Bsmt.Qual, 
                                       levels = c("Unknown", "Po", "Fa", 
                                                  "TA", "Gd", "Ex"))),
         Bsmt.Cond = fct_explicit_na(Bsmt.Cond, "Unknown"),
         Bsmt.Cond = as.numeric(factor(Bsmt.Cond, 
                                       levels = c("Unknown", "Po", "Fa", 
                                                  "TA", "Gd", "Ex"))),
         Bsmt.Exposure = fct_explicit_na(Bsmt.Exposure, "Unknown"),
         Bsmt.Exposure = as.numeric(factor(Bsmt.Exposure, 
                                           levels = c("Unknown", "No", "Mn", 
                                                      "Av", "Gd"))),
         BsmtFin.Type.1 = fct_explicit_na(BsmtFin.Type.1, "Unknown"),
         BsmtFin.Type.1 = as.numeric(factor(BsmtFin.Type.1, 
                                            levels = c("Unknown", "Unf","LwQ",
                                                       "Rec", "BLQ", 
                                                       "ALQ", "GLQ"))),
         BsmtFin.Type.2 = fct_explicit_na(BsmtFin.Type.2, "Unknown"),
         BsmtFin.Type.2 = as.numeric(factor(BsmtFin.Type.2, 
                                            levels = c("Unknown", "Unf","LwQ",
                                                       "Rec", "BLQ", 
                                                       "ALQ", "GLQ"))),
         Mas.Vnr.Area = ifelse(is.na(Mas.Vnr.Area), 0, Mas.Vnr.Area),
         Utilities = as.numeric(factor(Utilities, 
                                       levels = c("ELO","NoSeWa","NoSewr","AllPub"))),
         Lot.Shape = as.numeric(factor(Lot.Shape, 
                                       levels = c("IR3","IR2","IR1","Reg"))),
         Exter.Qual= as.numeric(factor(Exter.Qual, 
                                       levels = c("Po", "Fa", 
                                                  "TA", "Gd", "Ex"))),
         Land.Slope =  as.numeric(factor(Land.Slope, 
                                         levels = c("Sev", "Mod","Gtl"))),
         Exter.Cond = as.numeric(factor(Exter.Cond, 
                                        levels = c("Po", "Fa", 
                                                   "TA", "Gd", "Ex"))),
         Heating.QC = as.numeric(factor(Heating.QC, 
                                        levels = c("Po", "Fa", 
                                                   "TA", "Gd", "Ex"))),
         Electrical = as.numeric(factor(Electrical, 
                                        levels = c("Mix", "FuseP", 
                                                   "FuseF", "FuseA", "SBrkr"))),
         Kitchen.Qual = as.numeric(factor(Kitchen.Qual, 
                                          levels = c("Po", "Fa", 
                                                     "TA", "Gd", "Ex"))),
         Functional = as.numeric(factor(Functional, 
                                        levels = c("Sal", "Sev", 
                                                   "Maj2", "Maj1", "Mod",
                                                   "Min2","Min1","Typ"))),
         Paved.Drive = as.numeric(factor(Paved.Drive, 
                                         levels = c("N", "P","Y"))),
         Bsmt.Half.Bath = ifelse(is.na(Bsmt.Half.Bath), 0, Lot.Frontage),
         Bsmt.Full.Bath = ifelse(is.na(Bsmt.Full.Bath), 0, Lot.Frontage)
  )

dat3 = dat2 %>%
  dplyr::select(-Condition.2)

train_clean = dat3[dat3$datset == "train",] %>%
  dplyr::select(-datset)
test_clean = dat3[dat3$datset == "test",] %>%
  dplyr::select(-datset)
validation_clean = dat3[dat3$datset == "valid",] %>%
  dplyr::select(-datset)
```


```{r echo = F}
check = function(pred, true_value){
  return(data.frame(RMSE = RMSE(pred[,1],true_value),
                    BIAS = BIAS(pred[,1],true_value),
                    maxDeviation = maxDeviation(pred[,1],true_value),
                    MeanAbsDeviation = MeanAbsDeviation(pred[,1],true_value),
                    Coverage = coverage(pred[,2], pred[,3], true_value)))
}
RMSE = function(y,pred) {
  rmse = sqrt(mean((y-pred)^2))
  return(rmse)
}

BIAS = function(pred, true_value){
  return(mean(pred-true_value))
}
maxDeviation = function(pred, true_value){
  return(max(abs(pred-true_value)))
}
MeanAbsDeviation = function(pred, true_value){
  return(mean(abs(pred-true_value)))
}
coverage = function(lwr,upr,true_value){
  mean(lwr<true_value & true_value<upr)
}
```

After cleaning and processing data carefully, we analyzed the distribution characteristics of variables and developed multiple models to assess how well they fit the data.

## **Data Cleaing and Processing**

We put the 3 data sets (training, testing and validation) together for data cleaning. For all variables with more than half observations “NA”, we delete the variable. For the remaining variables:

* **Categorical variables**

  Without NA: no changes
  
  With NA: add a new level “NA”

* **Nominal variables**

  Without NA: convert to numeric variables in the ascending order

  With NA: add a new level “NA”, and convert to numeric variables in the ascending order with “NA” lowest.
  
  “”: delete the observation(s) (Garage.Finish), or convert to most popular values (Basement Exposure).

* **Discrete variables**
  
  Without NA: no changes

  With NA: delete the variable (year), change to 0 (Bsmt Full Bath, Bsmt Half Bath)

* **Continuous variables**

  Without NA: no changes
  
  With NA: change to 0 (lot.Frontage), 

## **Exploratory data analysis**

After the data cleaning, there are 74 variables in the data set, and 1499 observations in the training set, 500 observations in the testing set, and 413 observations in the validation set.

* **Dependent variable: price**

\begin{figure}[h!]
\centering
\includegraphics[width = 0.9\textwidth]{price.png}
\caption{Distribution of the Dependent Variable}
\label{figure:1}
\end{figure}

From the distribution histogram of the variable `price` in the training data in Figure \ref{figure:1}, we can see its original values are highly skewed below its mean. In order to obtain a more normal distribution to fit the requirement of most models, we take log transformation of the variable, and from the histogram plot we can see that its distribution is more symmetric and less heavy-tailed.

* **Correlation between continuous variables**

\begin{figure}[h!]
\centering
\includegraphics[width = 0.7\textwidth]{corr.png}
\caption{Correlation between Continuous Variables}
\label{figure:2}
\end{figure}

From the correlation plot in Figure \ref{figure:2}, we can see that `price` is highly correlated with `area`, `Overall.Qual`, `TotalSq`, `Year.Built`,  `Year.Remod.Add`, `Exter.Qual`, `Bsmt.Qual`, `Total.Bsmt.SF`, `X1stFlr.SF`, `Full.Bath`, `Kitchen.Qual`, `Fireplaces`, `Garage.Finish`, `Garage.Cars` and `Garage.Area`. In this way, in the model developing period, we can pay attention to these variables and consider a set of possible explanatory variables out of them.

Besides the relationship between the explanatory variables and the dependent variable, there are some evident possible correlations among explanatory variables, i.e., `area` and `TotalSq`, `Lot.Frontage` and `Bsmt.Full.Bath`, `Lot.Frontage` and `Bsmt.Half.Bath`, `Bsmt.Full.Bath` and `Bsmt.Half.Bath`, `Garage.Cond` and `Garage.Qual`, `Garage.Cars` and `Garage.Area`, `BsmtFin.SF.2` and `BsmtFin.Type.2`. With strong correlation between these variables, when some variable pair occur together in a model, we should check that there is no potential problems of multicollinearity.

* **Measurement of variable importance from different models**

We employed random forest and boosting to obtain a more reliable measurement of variable importance. In both rough models, we used the processed training data and all variables. From the following Figure \ref{figure:3} and Table \ref{table:1}, we can see that from both measures, `Overall.Qual`, `Neighborhood`, `TotalSq`, `area`, `Total.Bsmt.SF`, `Garage.Area`, `Exter.Qual`, `Garage.Cars`, `Bsmt.Qual`, `BsmtFin.SF.1`, `Kitchen.Qual`, `X1st.Flr.SF`, `MS.SubClass` and `Lot.Area` are all variables which are considered as top 10 important in each model.

\begin{figure}[h!]
\centering
\includegraphics[height = 0.5\textheight]{rf_varImportance.png}
\caption{Variable Importance from Random Forest (top 10)}
\label{figure:3}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{|| c | c | c | c | c | c ||}
  \hline
  Variable & Overall.Qual & Neighborhood & TotalSq & area & Total.Bsmt.SF \\ \hline
  Relative Influence & 27.5692 & 17.3513 & 11.8593 & 6.2747 & 5.1457 \\ \hline \hline
  Variable & Garage.Area & BsmtFin.SF.1 & Kitchen.Qual & X1st.Flr.SF & MS.SubClass \\ \hline
  Relative Influence & 3.6117 & 2.7965 & 2.3262 & 2.3201 & 2.0250 \\ \hline
\end{tabular}
\caption{Variable Importance from Boosting (top 10)}
\label{table:1}
\end{table}

## Development and assessment of an initial model from Part I

* **Initial model**

The simple model with the best performance in RMSE is the OLS model with BIC and common sense variable selection.

The summary of the final simple model is listed as following. The adjusted R-square of the model is 0.9443.

```{r ols trans x and y}
model_ols_trans = lm(log(price) ~ 
                       area + log(Lot.Area + 1) + Neighborhood + Bldg.Type + 
                       Overall.Qual + Overall.Cond + 
                       Year.Built + 
                       # Year.Remod.Add + 
                       Bsmt.Exposure + BsmtFin.SF.1 + BsmtFin.SF.2 + Bsmt.Unf.SF +
                       Central.Air + 
                       Kitchen.Qual + Functional + 
                       Fireplaces + 
                       Garage.Cars + 
                       # Garage.Area + 
                       Paved.Drive + Open.Porch.SF, 
                       #+ log(Enclosed.Porch + 1) + log(Screen.Porch + 1), 
                       data = train_clean[-c(168,183,462),])
summary(model_ols_trans)
```

From the estimated coefficients and their significance levels, we can see that among continous variables, `area`, `log(Lot.Area + 1)`, `BsmtFin.SF.1`, `BsmtFin.SF.2`, `Bsmt.Unf.SF` and `Open.Porch.SF` are significantly positive. This means that when one of these variables increases, the sale price of the house increases as well.

Among ordinal variables, `Overall.Qual`, `Overall.Cond`, `Bsmt.Exposure`, `Kitchen.Qual`, `Functional`, `Fireplaces`, `Garage.Cars` and `Paved.Drive` are significantly positive. This means that for these variables, when their levels get higher, the sale price of the house increases as well.

Among categorical variables, for `Neighborhood`, some neighborhood areas have significantly lower house prices, some areas have significantly higher house prices, which some areas are not so affective to house prices. For `Bldg.Type`, sale prices of houses of types "duplex" and "townhouse end unit" are significantly lower than those of houses of the type "single-family detached". For `Central.Air`, sales prices of houses with central air are significantly higher than those of houses without central air.

Considering the mechanism of the housing market, we consider the predictive results on these variables reasonable and close to reality.

* **Model selection**

To obtain the model, we have tried among OLS, OLS and BIC, random forest, boosting, ridge and lasso models. By comparing the performance in terms of RMSE on both the training and testing data, we find that the OLS and BIC model is the best one, with smallest discrepency between two data sets, and smallest RMSE on the testing data too.

We first put all explanatory variables into the linear model. During this step, we find out from the diagnostic plots that there are several outliers with extreme values (index 168, 183, 462 in the training set) and we delete them to fit the same model again. 

Then we do BIC variable selection on the output model. There are 22 variables (except for the intercept) in the selected model with minimal BIC value.

From the termplot of the minimal-BIC ols model, we detect that some variables (`Lot.Area`, `Enclosed.Porch`, and `Screen.Porch`) need log transformations to better fit the data trend. And after taking the meanings and correlations of variables into consideration, we decide to remove 4 variable (`Year.Remod.Add`, `Garage.Area`, `log(Enclosed.Porch+1)` and `log(Screen.Porch + 1)`) from the model. 

* **Residual**

```{r residual plot, echo = F}
par(mfrow = c(2,2))
plot(model_ols_trans)
```

From the residual plots of the selected model, we can get some measurement of the model.

In the Residual-Fitted plot, for all fitted values, the regression residuals are pretty close to 0, and there is little variation in residuals. This means that the model complies with the assumption that the mean of residuals are 0 and same for all observations very well.

In the Normal Q-Q plot, there are signs of left skewness in the response variable.

In the Scale-Location plot, there are still observations with squared standardized residuals close to 2.0, indicating that they may still be outliers and needs further improvement.

In the leverage plot, from the output warning message, there are observations with leverage value close to 1, indicating that they may be influential points and needs further improvement. We have tried to remove the outlier point and it turns out that new outliers showing up continuously. We regard this as a sign for us to transform and improve the model structure in the sophisticated models.

* **RMSE**

```{r RMSE for OLS-BIC-transformed model, echo=FALSE}
suppressMessages(library(xtable))
predict_ols_trans_train = exp(model_ols_trans$fitted.values)
rmse_ols_trans_train = RMSE(predict_ols_trans_train, train_clean[-c(168,183,462),]$price)

predict_ols_trans_test = predict(model_ols_trans, newdata = test_clean)
predict_ols_trans_test = exp(predict_ols_trans_test)
rmse_ols_trans_test = RMSE(predict_ols_trans_test, test_clean$price)
```

\begin{table}[h!]
\centering
\begin{tabular}{|| c | c | c ||}
  \hline
  & RMSE\_train & RMSE\_test \\ \hline
  values & 15493.6357 & 15735.0644 \\  \hline
\end{tabular}
\caption{RMSE of the OLS-BIC transformed model}
\label{table:2}
\end{table}

From Table \ref{table:2}, we can see that the RMSE for the chosen simple model is 15493.63 on the training data, and 15735.06 on the testing data. From the comparison between two RMSE values, we can tell that there is no overfitting or lack-of-fitting in the proposed model. 

* Model testing

Regression formula: 

\begin{equation}
\begin{split}
		\log(price) = (intercept) + area + \log(Lot.Area + 1) + Neighborhood + Bldg.Type \\
		+ Overall.Qual + Overall.Cond + Year.Built + Bsmt.Exposure + BsmtFin.SF.1 \\
		+ BsmtFin.SF.2 + Bsmt.UnF.SF + Central.Air + Kitchen.Qual + Functional \\
		+ Fireplaces + Garage.Cars + Paved.Drive + Open.Porch.SF
\end{split}
\end{equation}

On the first observation of the training data:

$X = (1, \log(4960+1), BrkSide, 1Fam, 5, 7, 1930, 2, 0, 0, 297, Y, 3, 8, 1, 1, 1, 60 )$

\begin{equation}
\begin{split}
  \hat{\beta} = (3.925, 2.673*10^{-2}, 9.137 * 10^{-2}, -2.217*10^{-2}, 0, 5.589*10^{-2}, 4.096*10^{-2}, \\
  2.877*10^{-3}, 1.121*10^{-2}, 1.823*10^{-4}, 1.367*10^{-4}, 8.529*10^{-5}, 5.653*10^{-2}, 3.870*10^{-2}, \\
  2.603*10^{-2}, 2.328*10^{-2}, 3.726*10^{-2}, 1.594*10^{-2}, 1.236*10^{-4})
\end{split}
\end{equation}

After calculation, we can get $X*\hat{\beta} = 11.7535$, $\exp(11.7535) = 127,196.2$, and the true value of `price` in the first observation is $137,000$. The residual value is $137000 - 127196.2 = 9803.8$. We regard this error as a reasonable one.

On the first observation of the testing data:

$X = (1, \log(11727+1), NAmes, 1Fam, 7, 6, 1969, 3, 0, 0, 1851, Y, 3, 8, 1, 2, 3, 146 )$

\begin{equation}
\begin{split}
  \hat{\beta} = (3.925, 2.673*10^{-2}, 9.137 * 10^{-2}, -7.803*10^{-2}, 0, 5.589*10^{-2}, 4.096*10^{-2}, \\
  2.877*10^{-3}, 1.121*10^{-2}, 1.823*10^{-4}, 1.367*10^{-4}, 8.529*10^{-5}, 5.653*10^{-2}, 3.870*10^{-2}, \\
  2.603*10^{-2}, 2.328*10^{-2}, 3.726*10^{-2}, 1.594*10^{-2}, 1.236*10^{-4})
\end{split}
\end{equation}

After calculation, we can get $X*\hat{\beta} = 12.2354$, $\exp(12.2354) = 205952.3$, and the true value of `price` in the first observation is $192,100$. The residual value is $192100 - 205952.3 = -13852.3$. We regard this error as a reasonable one.

## Development of the final model

* **Final model**

  We decided to start with a full model with all available predictors entered. We have tried GAM, boosting, random forest, and BMA, most of them have overfitting issues which lead to poor predictions on test set. To prevent from overfitting using so many predictors, we decided to fit a LASSO linear regression to control the size of the coefficients and do some free variable selections. Since there are some multicolinearity issue, we deleted column `Exterior.1st`,`Exterior.2nd`,`Roof.Matl`,`X1st.Flr.SF`,`X2nd.Flr.SF`,`Total.Bsmt.SF`,`Low.Qual.Fin.SF`.  To better fit the data, we included interaction terms between `area` and all the factor predictors, since according to common sense, the extra square feet in different region or neighbourhood has different value. Here is the coefficients of all the non-zero predictors.

```{r process variables and get the model, echo=FALSE}
suppressMessages(library(MASS))
suppressMessages(library(knitr))

set.seed(1)
delete_list = c(23,24,25,39,44,45,46)

cont_var = setdiff(names(sapply(train_clean[,-delete_list],class))[sapply(train_clean[,-delete_list],class)!="factor"],c("PID","price"))
factor_var = names(sapply(train_clean[,-delete_list],class))[sapply(train_clean[,-delete_list],class)=="factor"]

interac = paste("area",paste(":",factor_var))
# formula for lasso
fmla = as.formula(paste("log(price) ~ Neighborhood:area+", paste(cont_var, collapse= "+"),"+",paste(factor_var,collapse = "+"),"+",paste(interac,collapse = "+")))

suppressMessages(library(glmnet))

# use cross validation to choose the best lambda
model_lasso = cv.glmnet(model.matrix(fmla,train_clean)[,-1], 
                      log(train_clean$price), 
                      alpha=1,
                      lambda= 10^seq(4,-3,length= 1000))
model_lasso.lambda.best = model_lasso$lambda.min

tmp_coeffs <- coef(model_lasso, s = "lambda.min")
df = data.frame(name = tmp_coeffs@Dimnames[[1]][tmp_coeffs@i+1], coefficient = tmp_coeffs@x)
kable(df)
```

```{r prediction on train data, echo=FALSE}
predict_lasso_train = predict(model_lasso, 
                      model.matrix(fmla, train_clean)[,-1],
                      s = model_lasso.lambda.best)
predict_lasso_train = exp(predict_lasso_train)
rmse_lasso_train = RMSE(predict_lasso_train, train_clean$price)
```

```{r prediction on test data, echo=FALSE}
predict_lasso_test = predict(model_lasso, 
                      model.matrix(fmla,test_clean)[,-1],
                      s = model_lasso.lambda.best)
predict_lasso_test = exp(predict_lasso_test)
rmse_lasso_test = RMSE(predict_lasso_test, test_clean$price)
```

```{r confidence interval on test data, echo=FALSE}
nsim = 50
predict_lasso_test_multiple = matrix(0, nrow(test_clean), nsim)

for(i in 1:nsim){
  model_lasso_temp = cv.glmnet(model.matrix(fmla,train_clean)[,-1],
                      log(train_clean$price),
                      alpha=1,
                      lambda= 10^seq(4,-3,length= 1000))
  model_lasso_temp.lambda.best = model_lasso_temp$lambda.min

  predict_lassp_temp = predict(model_lasso_temp,
                      model.matrix(fmla,test_clean)[,-1],
                      s = model_lasso_temp.lambda.best)
  predict_lasso_test_multiple[,i] = predict_lassp_temp
}

quantile_lasso_test = apply(exp(predict_lasso_test_multiple), 1, quantile, c(0.025,0.975))
coverage_lasso = coverage(quantile_lasso_test[1,], quantile_lasso_test[2,], test_clean$price)
```

* **Variables**

The remaining continuous variable includes `area`,`Lot.Area`,`Lot.Frontage`,`Lot.Area`,`Overall.Qual`,`Overall.Cond`,`Year.Built`,
`Year.Remod.Add`,`Mas.Vnr.Area`,`Exter.Qual`,`Exter.Cond`,`Bsmt.Cond`,`Bsmt.Exposure`,
`BsmtFin.Type.1`,`BsmtFin.SF.1`,`BsmtFin.SF.2`,`smt.Unf.SF`,`Heating.QC`,`Electrical`,
`Bsmt.Full.Bath`,`Bsmt.Half.Bath`,`Full.Bath`,`alf.Bath`,`Kitchen.AbvGr`,`Kitchen.Qual`,
`Functional`,`Fireplaces`,`Garage.Finish`,`Garage.Cars`,`Garage.Area`,`Garage.Qual`,
`Garage.Cond`,etc. Basically, ordinal data and area data remained in the equation. Categorical data like `neighborhood`,`Bldg.Type`,etc remained in the equation as usual.

* **Variable selection/shrinkage**

The penalizing constant we picked was $`r model_lasso.lambda.best`$ which was determined by cross validation. Under this regularization, the number of predictors reduced from 272 to 109 by using LASSO. As predicted, location variables like `neighborhood` and the its interaction with `area` remained in the model. 

## Assessment of the final model

* **Residual**

```{r, echo=FALSE}
residual = scale(predict_lasso_train - train_clean$price)

ggplot(data.frame(cbind(residual, train_clean$price)), aes(x= predict_lasso_train, y=residual)) +
    geom_point(shape=1)  +
    ggtitle("residual plot for Lasso Regression") +
    labs(x="fitted value for price",y="standardize residual") 
```

Since we can not use package to plot residuals of Lasso Regression, we plot manually. First, we calculate the residual of training data as y values for the plot. Residuals equal fitted value of training data minius true value, and then we standardized the residual. Our plot has residaul as y values and fitted price as x values. The standardized residual plot shows that most residual are within -3 and +3, and Most of points are between evenly distributed.

* **RMSE and coverage**

RMSE value for training data using simple model is $`r rmse_ols_trans_train`$, and for test data using simple model is $`r rmse_ols_trans_test`$.

RMSE value for training data using Lasso Regression with selected interactions is $`r rmse_lasso_train`$ and for test data with selected interactions is $`r rmse_lasso_test`$. 

Compared to the simple model, RMSE for both trainig data and test data using Lasso Regression are smaller, which means both bias and variance decrease in the complex model. 

The result shows that the coverage for Lasso Regression is $`r coverage_lasso`$. Since Lasso Regression is OLS with L2 penalizaiton, which leads to bias with lower variance. So, we get really low coverage due to low variance.

* **Model evaluation**

To predict price of housing, we constructed multiple linear regression with stepwise BIC, Ridge Reression, Lasso Regression, Random Forest, boosting and BMA. Though linear regression with stepwise BIC can do variable selection, its RMSE is relative higher. Ridge regression cannot do variable selection. Random Forest, boosting and BMA show a lack of interpretibility. Compared to RMSEs of other models, we found Lasso has the smallest RMSE. Also Lasso is able to do variable selection to help us with selecting predictors. So we choose Lasso regression as the final model. 

* **Model result**

Show 10 most overvalued and undervalued houses.

Overvalued: 

```{r, echo=FALSE}
set.seed(2)
diff = data.frame(predict_lasso_test - test_clean$price)
diff = cbind(test_clean$PID, diff)
overvalued = diff %>% 
  arrange(desc(X1)) %>%
  head(.,10)
colnames(overvalued) = c("PID", "Difference")
kable(overvalued)
```

Undervalued:

```{r, echo = FALSE}
undervalued = diff %>% 
  arrange(X1) %>%
  head(.,10)
colnames(undervalued) = c("PID", "Difference")
kable(undervalued)
```

## Conclusions

* **Summary of results** 

We found linear regression only with three predictors `TotalSq`, `Total.Bsmt.SF`, `Neighborhood` could explained around 82% of the training data. The model we got by using stepwise BIC, which filtered out 19 variables from 82 variables, only improve it to 94%. It suggested `TotalSq`, toEven this model explained more training data and got a better RMSE on testing data. We still doubted this model may be to complicated.

We have used multiple linear regression with stepwise BIC, Ridge regression, Lasso regression, Poisson regression, Random forest, boosting, BMA to fit the training data and compared RMSEs both from training data and testing data. We finally found Lasso regression comes with the best testing and training RMSEs.

* **Things learned**

1. The first and most important part of data analysis would be data cleaning. We spent much time on cleaning data, including NA imputation, ordinal variable transfermation to reasonable numerical variable, and factors defination. Only with proper data cleaning, we can start models building.

2. Before fitting any models, we have to fully understand the data. We looked through the description of all variables, and wrote R script to summarize features of different variables. More deeply we can understand data, more possiblly we can make proper data transformation and create innovative interactions.

3. We tried through all statistical models we have learned from STA521, and we learned that complicated and advanced models don't guarantee better predictions. The multivariable linear regression with simple stepwise BIC generated better preditions than most advanced and complicated model, which was beyond our expectation.
